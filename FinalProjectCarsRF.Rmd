---
title: "RandomForestCars"
author: "Griffin Sheppard"
date: "2023-12-03"
output: html_document
---

```{r, message=FALSE, warning=FALSE}
library(tree)
library(maptree)
library(tidymodels)
library(tidyverse)
library(xgboost)
library(glmnet)
library(ISLR)
library(ISLR2)
library(ranger)
library(vip)


data(Auto)

set.seed(10)
```

```{r}
#converting origin to a factor
auto <- tibble(ISLR::Auto) %>% 
  mutate(origin = factor(origin))

#splitting
auto_split <- initial_split(auto, strata=mpg, prop=0.8)
auto_train <- training(auto_split)
auto_test <- testing(auto_split)

#5-fold cross validation
auto_folds <- vfold_cv(auto_train, v = 5, strata = "mpg")
```


```{r}
# Decision tree for Auto dataset
tree.auto = tree(mpg ~ . -name, data = auto_train)
plot(tree.auto)
text(tree.auto, pretty=0, col = "purple", cex = .5)
title("Pruned tree of Size 3")
```

```{r}
# 10-fold cross-validation to select the best size of a tree which minimizes the cross-validation estimate of the test error rate

cv.auto <- cv.tree(tree.auto, K=10)

best_size <- min(cv.auto$size[cv.auto$dev == min(cv.auto$dev)])
best_size
```

```{r}
pt.cv = prune.tree(tree.auto, best=best_size)

draw.tree(pt.cv, nodeinfo=F)
```

```{r}
#making recipe
recipe_auto <- recipe(mpg ~., data=auto_train) %>% 
  step_rm(name) %>% #remove name of vehicle
  step_dummy(all_nominal_predictors()) %>% 
  step_normalize(all_predictors())

#making random forest
rf_auto <- rand_forest(mtry = tune(), #num of preds randomly sampled at each split
                           trees = tune(), #num of trees
                           min_n = tune()) %>% # min num of data point in a node
  set_engine("ranger", importance = "impurity") %>% 
  set_mode("regression")

#making workflow
rf_auto_wf <- workflow() %>% 
  add_model(rf_auto) %>% 
  add_recipe(recipe_auto)
```


```{r}
#making grid of hyperparameter values to consider
rf_grid_auto <- grid_regular(mtry(range = c(1, 8)), 
                        trees(range = c(200, 600)),
                        min_n(range = c(10, 20)),
                        levels = 5)
#fit RF models
tune_auto <- tune_grid(
  rf_auto_wf, 
  resamples = auto_folds, 
  grid = rf_grid_auto)

#save results
save(tune_auto, file="tune_auto.rda")
```


```{r, warning=FALSE}
#load results
load("tune_auto.rda")

#plot of hyperparameter performance metrics
autoplot(tune_auto) + theme_minimal()

#show top 5 RFs
show_best(tune_auto, n=5)

#save best RF
best_rf_auto <-select_best(tune_auto)

#finalize/fit best RF
final_auto_model <- finalize_workflow(rf_auto_wf, best_rf_auto)
final_auto_model <- fit(final_auto_model, auto_train)
```
The model seems to have better performance with a smaller minimum node size, the number of trees doesn't appear to affect the performance and the performance of the model plateaus after 4 predictors. The best performing model had 8 randomly sampled predictors, 300 trees and a minimum of 10 data points in a node. It had a mean RMSE of 2.906.


```{r}
#graph of most important predictors
final_auto_model %>% extract_fit_parsnip() %>% 
  vip() +
  theme_minimal()

#RMSE of testing data
final_auto_model_test <- augment(final_auto_model, auto_test)
yardstick::rmse(final_auto_model_test, truth=mpg, .pred)

#scatterplot of actual mpg vs predicted
final_auto_model_test %>% 
  ggplot(aes(x = mpg, y = .pred)) +
  geom_point(alpha = 0.5) +
  theme_minimal()
```